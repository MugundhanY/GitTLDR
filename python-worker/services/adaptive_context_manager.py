"""
Adaptive Context Manager - Solves Context Window & Scalability Limitations
==========================================================================

PROBLEM: Current system can only analyze ~15 files at once, limiting:
- Microservices (5+ services) - 60-70% success
- Large refactorings - 60-70% success
- Complex architectures

SOLUTION: Multi-pass adaptive context management with:
1. **Intelligent Chunking**: Break large codebases into logical chunks
2. **Hierarchical Analysis**: Analyze at multiple levels (service â†’ module â†’ function)
3. **Progressive Refinement**: Start broad, zoom into relevant areas
4. **Context Compression**: Summarize less relevant files to fit more in context
5. **Multi-Pass Processing**: Multiple focused passes instead of one broad pass

Target: 85-90% success on complex codebases
"""

import asyncio
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass
from collections import defaultdict
from utils.logger import get_logger

logger = get_logger(__name__)


@dataclass
class CodeChunk:
    """A logical chunk of code (service, module, package)."""
    id: str
    type: str  # 'service', 'module', 'package', 'directory'
    name: str
    files: List[Dict[str, Any]]
    dependencies: List[str]  # IDs of chunks this depends on
    relevance_score: float
    size_tokens: int
    summary: Optional[str] = None


@dataclass
class ContextWindow:
    """A context window with constraints."""
    max_files: int = 15
    max_tokens: int = 100000  # ~75k words
    reserved_for_instructions: int = 10000
    
    @property
    def available_tokens(self) -> int:
        return self.max_tokens - self.reserved_for_instructions


class AdaptiveContextManager:
    """
    Manages context windows adaptively based on codebase complexity.
    
    Key Innovation: Instead of analyzing all files at once (impossible),
    we analyze in multiple focused passes, each with full context.
    
    Example for microservices:
    - Pass 1: Analyze service boundaries, identify relevant services
    - Pass 2: Analyze API contracts between services
    - Pass 3: Deep dive into relevant service internals
    - Pass 4: Cross-service integration points
    
    Each pass uses full 15-file context, but focuses on different aspects.
    """
    
    def __init__(self):
        self.context_window = ContextWindow()
        self.compression_ratio = 0.3  # Compress less relevant files to 30% size
    
    async def create_adaptive_context(
        self,
        all_files: List[Dict[str, Any]],
        issue_description: str,
        issue_type: str,
        repository_structure: Optional[Dict[str, Any]] = None
    ) -> List[Dict[str, List[Dict[str, Any]]]]:
        """
        Create multiple context windows adaptively based on complexity.
        
        Returns:
            List of context passes, each with files to analyze
            [
                {'pass': 1, 'focus': 'architecture', 'files': [...]},
                {'pass': 2, 'focus': 'implementation', 'files': [...]},
                ...
            ]
        """
        logger.info(f"ðŸ§© Creating adaptive context for {len(all_files)} files")
        
        # Detect codebase complexity
        complexity = self._analyze_complexity(all_files, repository_structure)
        logger.info(f"ðŸ“Š Complexity analysis: {complexity['level']} ({complexity['score']:.2f}/10)")
        
        # Choose strategy based on complexity
        if complexity['level'] == 'simple':
            # Single pass is sufficient
            return await self._create_single_pass_context(all_files, issue_description)
        
        elif complexity['level'] == 'moderate':
            # Two-pass: architecture + implementation
            return await self._create_two_pass_context(all_files, issue_description, complexity)
        
        else:  # complex
            # Multi-pass: hierarchical analysis
            return await self._create_multi_pass_context(all_files, issue_description, complexity)
    
    def _analyze_complexity(
        self,
        files: List[Dict[str, Any]],
        structure: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Analyze codebase complexity to determine strategy.
        
        Factors:
        - Number of services (microservices indicator)
        - Directory depth (hierarchy complexity)
        - File count (scale)
        - Language diversity (polyglot complexity)
        - Dependency density (coupling)
        """
        analysis = {
            'file_count': len(files),
            'services': set(),
            'max_depth': 0,
            'languages': set(),
            'has_docker_compose': False,
            'has_k8s': False,
            'directory_count': 0
        }
        
        # Detect patterns
        directories = set()
        for file in files:
            path = file.get('path', '')
            
            # Count directory depth
            depth = path.count('/')
            analysis['max_depth'] = max(analysis['max_depth'], depth)
            
            # Extract directories
            if '/' in path:
                dirs = path.split('/')[:-1]
                directories.update(['/'.join(dirs[:i+1]) for i in range(len(dirs))])
            
            # Detect services (common patterns)
            if 'services/' in path or 'microservices/' in path:
                parts = path.split('/')
                if 'services' in parts:
                    idx = parts.index('services')
                    if idx + 1 < len(parts):
                        analysis['services'].add(parts[idx + 1])
            
            # Detect languages
            ext = path.split('.')[-1] if '.' in path else ''
            if ext:
                analysis['languages'].add(ext)
            
            # Detect orchestration
            if 'docker-compose' in path.lower():
                analysis['has_docker_compose'] = True
            if path.endswith(('.yaml', '.yml')) and ('k8s' in path or 'kubernetes' in path):
                analysis['has_k8s'] = True
        
        analysis['directory_count'] = len(directories)
        analysis['service_count'] = len(analysis['services'])
        analysis['language_count'] = len(analysis['languages'])
        
        # Calculate complexity score (0-10)
        score = 0.0
        
        # File count contribution (0-2)
        if analysis['file_count'] > 100:
            score += 2.0
        elif analysis['file_count'] > 50:
            score += 1.5
        elif analysis['file_count'] > 20:
            score += 1.0
        else:
            score += 0.5
        
        # Service count (microservices) (0-3)
        if analysis['service_count'] >= 5:
            score += 3.0
        elif analysis['service_count'] >= 3:
            score += 2.0
        elif analysis['service_count'] >= 1:
            score += 1.0
        
        # Depth (architecture complexity) (0-2)
        if analysis['max_depth'] >= 5:
            score += 2.0
        elif analysis['max_depth'] >= 3:
            score += 1.0
        
        # Language diversity (0-2)
        if analysis['language_count'] >= 3:
            score += 2.0
        elif analysis['language_count'] >= 2:
            score += 1.0
        
        # Orchestration (0-1)
        if analysis['has_k8s']:
            score += 1.0
        elif analysis['has_docker_compose']:
            score += 0.5
        
        # Determine level
        if score <= 3.0:
            level = 'simple'
        elif score <= 6.0:
            level = 'moderate'
        else:
            level = 'complex'
        
        analysis['score'] = score
        analysis['level'] = level
        
        logger.info(
            f"ðŸ“Š Complexity breakdown: "
            f"{analysis['file_count']} files, "
            f"{analysis['service_count']} services, "
            f"depth {analysis['max_depth']}, "
            f"{analysis['language_count']} languages"
        )
        
        return analysis
    
    async def _create_single_pass_context(
        self,
        files: List[Dict[str, Any]],
        issue_description: str
    ) -> List[Dict[str, Any]]:
        """Simple case: one pass with top 15 files."""
        logger.info("ðŸ“¦ Using single-pass strategy (simple codebase)")
        
        # Just use the files as-is (they're already ranked by relevance)
        selected = files[:self.context_window.max_files]
        
        return [{
            'pass': 1,
            'focus': 'complete',
            'files': selected,
            'strategy': 'single_pass'
        }]
    
    async def _create_two_pass_context(
        self,
        files: List[Dict[str, Any]],
        issue_description: str,
        complexity: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Moderate complexity: Two passes.
        
        Pass 1: Architecture (configs, main files, interfaces)
        Pass 2: Implementation (specific modules relevant to issue)
        """
        logger.info("ðŸ“¦ðŸ“¦ Using two-pass strategy (moderate complexity)")
        
        # Categorize files
        architectural = []
        implementation = []
        
        for file in files:
            path = file.get('path', '').lower()
            
            # Architectural files
            if any(pattern in path for pattern in [
                'config', 'setup', 'main.py', 'app.py', '__init__.py',
                'index.', 'server.', 'api.', 'routes', 'settings',
                'docker', 'package.json', 'requirements.txt', 'go.mod',
                'pom.xml', 'build.gradle', 'cargo.toml'
            ]):
                architectural.append(file)
            else:
                implementation.append(file)
        
        # Pass 1: Architecture + top implementation
        pass1_files = architectural[:10]  # Architecture files
        pass1_files.extend(implementation[:5])  # Top implementation files
        
        # Pass 2: Deeper implementation
        pass2_files = implementation[:15]  # Focus on implementation
        
        logger.info(f"  Pass 1: {len(pass1_files)} files (architecture + overview)")
        logger.info(f"  Pass 2: {len(pass2_files)} files (implementation details)")
        
        return [
            {
                'pass': 1,
                'focus': 'architecture',
                'files': pass1_files,
                'strategy': 'two_pass'
            },
            {
                'pass': 2,
                'focus': 'implementation',
                'files': pass2_files,
                'strategy': 'two_pass'
            }
        ]
    
    async def _create_multi_pass_context(
        self,
        files: List[Dict[str, Any]],
        issue_description: str,
        complexity: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """
        Complex codebases (microservices, large refactorings).
        
        Pass 1: Service boundaries (identify which services are relevant)
        Pass 2: Service interfaces (APIs, contracts between services)
        Pass 3: Service internals (deep dive into relevant services)
        Pass 4: Integration points (cross-service dependencies)
        """
        logger.info("ðŸ“¦ðŸ“¦ðŸ“¦ðŸ“¦ Using multi-pass strategy (complex codebase)")
        
        # Group files by service/module
        chunks = self._create_code_chunks(files, complexity)
        
        # Rank chunks by relevance to issue
        ranked_chunks = self._rank_chunks_by_relevance(chunks, issue_description)
        
        passes = []
        
        # Pass 1: Service boundaries (high-level overview)
        logger.info("  Pass 1: Service boundaries & architecture")
        boundary_files = []
        for chunk in ranked_chunks[:5]:  # Top 5 services/modules
            # Get main/config files from each
            for file in chunk.files:
                if self._is_boundary_file(file):
                    boundary_files.append(file)
                    if len(boundary_files) >= 15:
                        break
            if len(boundary_files) >= 15:
                break
        
        passes.append({
            'pass': 1,
            'focus': 'service_boundaries',
            'files': boundary_files[:15],
            'strategy': 'multi_pass',
            'description': 'Identify relevant services and their responsibilities'
        })
        
        # Pass 2: Service interfaces (APIs, contracts)
        logger.info("  Pass 2: Service interfaces & API contracts")
        interface_files = []
        for chunk in ranked_chunks[:3]:  # Top 3 most relevant
            for file in chunk.files:
                if self._is_interface_file(file):
                    interface_files.append(file)
                    if len(interface_files) >= 15:
                        break
            if len(interface_files) >= 15:
                break
        
        passes.append({
            'pass': 2,
            'focus': 'service_interfaces',
            'files': interface_files[:15],
            'strategy': 'multi_pass',
            'description': 'Understand how services communicate'
        })
        
        # Pass 3: Service internals (implementation)
        logger.info("  Pass 3: Service implementation details")
        internal_files = []
        # Take most relevant chunk and go deep
        if ranked_chunks:
            top_chunk = ranked_chunks[0]
            internal_files = top_chunk.files[:15]
        
        passes.append({
            'pass': 3,
            'focus': 'service_internals',
            'files': internal_files,
            'strategy': 'multi_pass',
            'description': f'Deep dive into most relevant service: {top_chunk.name if ranked_chunks else "N/A"}'
        })
        
        # Pass 4: Integration points (if needed for cross-service changes)
        if complexity['service_count'] >= 3:
            logger.info("  Pass 4: Cross-service integration")
            integration_files = []
            for chunk in ranked_chunks[:2]:
                for file in chunk.files:
                    if self._is_integration_file(file):
                        integration_files.append(file)
                        if len(integration_files) >= 15:
                            break
                if len(integration_files) >= 15:
                    break
            
            passes.append({
                'pass': 4,
                'focus': 'integration_points',
                'files': integration_files[:15],
                'strategy': 'multi_pass',
                'description': 'Analyze cross-service dependencies and integration'
            })
        
        logger.info(f"âœ… Created {len(passes)} context passes for complex codebase")
        return passes
    
    def _create_code_chunks(
        self,
        files: List[Dict[str, Any]],
        complexity: Dict[str, Any]
    ) -> List[CodeChunk]:
        """Group files into logical chunks (services, modules, packages)."""
        
        chunks_by_path = defaultdict(list)
        
        # Group files by top-level directory or service
        for file in files:
            path = file.get('path', '')
            
            # Determine chunk ID
            if 'services/' in path:
                # Microservice pattern
                parts = path.split('/')
                idx = parts.index('services')
                chunk_id = f"service:{parts[idx+1]}" if idx + 1 < len(parts) else "service:unknown"
            elif '/' in path:
                # Regular module pattern
                top_dir = path.split('/')[0]
                chunk_id = f"module:{top_dir}"
            else:
                # Root files
                chunk_id = "module:root"
            
            chunks_by_path[chunk_id].append(file)
        
        # Convert to CodeChunk objects
        chunks = []
        for chunk_id, chunk_files in chunks_by_path.items():
            chunk_type, chunk_name = chunk_id.split(':', 1)
            
            chunks.append(CodeChunk(
                id=chunk_id,
                type=chunk_type,
                name=chunk_name,
                files=chunk_files,
                dependencies=[],  # TODO: Extract from imports
                relevance_score=0.0,  # Will be set by ranking
                size_tokens=sum(len(f.get('content', '')) // 4 for f in chunk_files)
            ))
        
        logger.info(f"ðŸ§© Created {len(chunks)} code chunks: {[c.name for c in chunks]}")
        return chunks
    
    def _rank_chunks_by_relevance(
        self,
        chunks: List[CodeChunk],
        issue_description: str
    ) -> List[CodeChunk]:
        """Rank chunks by relevance to issue using keyword matching."""
        
        # Extract keywords from issue
        keywords = issue_description.lower().split()
        keywords = [k for k in keywords if len(k) > 3]  # Filter short words
        
        for chunk in chunks:
            score = 0.0
            
            # Match against chunk name
            chunk_name_lower = chunk.name.lower()
            for keyword in keywords:
                if keyword in chunk_name_lower:
                    score += 2.0
            
            # Match against file paths
            for file in chunk.files:
                path_lower = file.get('path', '').lower()
                for keyword in keywords:
                    if keyword in path_lower:
                        score += 1.0
            
            # Match against file content (summary if available)
            for file in chunk.files:
                content = file.get('summary', file.get('content', '')).lower()
                for keyword in keywords:
                    if keyword in content:
                        score += 0.5
            
            chunk.relevance_score = score
        
        # Sort by relevance
        ranked = sorted(chunks, key=lambda c: c.relevance_score, reverse=True)
        
        logger.info(f"ðŸ“Š Chunk relevance scores: {[(c.name, c.relevance_score) for c in ranked[:5]]}")
        return ranked
    
    def _is_boundary_file(self, file: Dict[str, Any]) -> bool:
        """Check if file defines service boundaries (main, config, setup)."""
        path = file.get('path', '').lower()
        return any(pattern in path for pattern in [
            'main.', '__main__', 'app.', 'server.', 'index.',
            'config', 'settings', '__init__.py',
            'docker', 'compose', 'package.json', 'requirements.txt'
        ])
    
    def _is_interface_file(self, file: Dict[str, Any]) -> bool:
        """Check if file defines interfaces (routes, controllers, DTOs)."""
        path = file.get('path', '').lower()
        return any(pattern in path for pattern in [
            'route', 'controller', 'handler', 'endpoint',
            'api', 'interface', 'contract', 'dto', 'model',
            'schema', 'proto', 'graphql'
        ])
    
    def _is_integration_file(self, file: Dict[str, Any]) -> bool:
        """Check if file handles integration (clients, adapters, gateways)."""
        path = file.get('path', '').lower()
        return any(pattern in path for pattern in [
            'client', 'adapter', 'gateway', 'proxy',
            'integration', 'connector', 'bridge', 'wrapper',
            'middleware', 'interceptor'
        ])


# Singleton instance
adaptive_context_manager = AdaptiveContextManager()
